# ═══════════════════════════════════════════════════════════════════
# Configuration — Non-Longevous Dataset Builder
# ═══════════════════════════════════════════════════════════════════

project:
  name: "non_longevous_dataset"
  description: "Dataset builder for non-longevous individuals from 1000 Genomes Project"
  output_dir: "non_longevous_results"

# ═══════════════════════════════════════════════════════════════════
# Data Sources
# ═══════════════════════════════════════════════════════════════════

data_sources:
  # CSV file with 1000 Genomes metadata
  # Header: FamilyID,SampleID,FatherID,MotherID,Sex,Population,Superpopulation
  # Sex: 1=Male, 2=Female
  metadata_csv: "doc/1000_genomes_metadata.csv"  # Use example file for testing
  
  # Reference genome
  reference:
    fasta: "refs/GRCh38_full_analysis_set_plus_decoy_hla.fa"
    name: "GRCh38"
  
  # VCF files location (chromosome pattern)
  # {chrom} will be replaced by chromosome name (e.g., chr1, chr2, chrX)
  vcf_pattern: "/path/to/1000G_vcfs/1kGP_high_coverage_Illumina.{chrom}.filtered.SNV_INDEL_SV_phased_panel.vcf.gz"

# ═══════════════════════════════════════════════════════════════════
# Sample Selection
# ═══════════════════════════════════════════════════════════════════

sample_selection:
  # Selection level: "superpopulation" or "population"
  level: "superpopulation"
  
  # Samples per group (depending on level selected)
  # If level="superpopulation", specify samples per superpopulation
  # If level="population", specify samples per population
  samples_per_group: 2
  
  # Optional: specific groups to include (leave empty for all)
  # Examples for superpopulation: ["AFR", "EUR", "EAS", "SAS", "AMR"]
  # Examples for population: ["ACB", "ASW", "CEU", "YRI", etc.]
  include_groups: []
  
  # Optional: exclude specific groups
  exclude_groups: []
  
  # Sex filter: "all", "male", or "female"
  sex_filter: "all"
  
  # Random seed for reproducible sampling
  random_seed: 42

# ═══════════════════════════════════════════════════════════════════
# build_window_and_predict.py Parameters
# ═══════════════════════════════════════════════════════════════════

build_window_params:
  # Gene to analyze (use either gene or gene_id, not both)
  gene: "CYP2B6"           # HGNC gene symbol
  gene_id: null            # ENSEMBL gene ID (e.g., ENSG00000197894)
  
  # GTF annotation (optional, will use public URL if not provided)
  gtf_feather: null
  
  # Window size (must match AlphaGenome supported sizes)
  window_size: 1000000     # 1 Mb (default for AlphaGenome)
  
  # Haplotype options
  skip_h2: false           # Set true to only build H1 (faster)
  also_iupac: false        # Set true to also build IUPAC consensus
  
  # AlphaGenome prediction options
  predict: true            # Run AlphaGenome predictions
  
  # API key (will use environment variable if not provided)
  api_key: "${ALPHAGENOME_API_KEY}"
  
  # Output types (comma-separated)
  # Available: RNA_SEQ, CAGE, ATAC, CHIP_HISTONE, CHIP_TF, DNASE, PROCAP, etc.
  outputs: "RNA_SEQ,ATAC"
  
  # Ontology terms (tissue/cell types, comma-separated CURIEs)
  # Leave empty to use all tissues (WARNING: slow and memory intensive)
  # Examples:
  #   UBERON:0002107 (liver)
  #   UBERON:0000955 (brain)
  #   UBERON:0000948 (heart)
  #   CL:0002601 (senescent cell)
  ontology: "UBERON:0002107,UBERON:0000955"
  
  # Skip confirmation when requesting all tissues
  all_tissues: false

# ═══════════════════════════════════════════════════════════════════
# Pipeline Execution Steps
# ═══════════════════════════════════════════════════════════════════

pipeline:
  steps:
    # Step 1: Analyze CSV and print statistics
    analyze_metadata: true       # ENABLED BY DEFAULT
    
    # Step 2: Select samples according to selection criteria
    select_samples: false
    
    # Step 3: Validate VCF files exist for selected samples
    validate_vcfs: false
    
    # Step 4: Run build_window_and_predict.py for each sample
    run_predictions: false
    
    # Step 5: Generate summary report
    generate_report: false
  
  # Parallel processing
  parallel:
    enabled: true
    n_workers: 4              # Number of parallel processes
  
  # Resume from checkpoint
  resume_from_checkpoint: true
  checkpoint_file: "non_longevous_dataset_checkpoint.json"

# ═══════════════════════════════════════════════════════════════════
# Logging
# ═══════════════════════════════════════════════════════════════════

logging:
  level: "INFO"              # DEBUG, INFO, WARNING, ERROR
  log_file: "non_longevous_dataset.log"
  console_output: true

