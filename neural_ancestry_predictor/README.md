# Neural Ancestry Predictor

> **ðŸ§¬ PrediÃ§Ã£o de Ancestralidade usando Redes Neurais e Dados AlphaGenome**

Este mÃ³dulo implementa uma rede neural configurÃ¡vel via YAML que prediz ancestralidade (superpopulation, population ou FROG likelihood) a partir de prediÃ§Ãµes AlphaGenome armazenadas em um dataset PyTorch.

## ðŸ“‘ Ãndice

- [VisÃ£o Geral](#visÃ£o-geral)
- [InstalaÃ§Ã£o](#instalaÃ§Ã£o)
- [Uso RÃ¡pido](#uso-rÃ¡pido)
- [ConfiguraÃ§Ã£o](#configuraÃ§Ã£o)
- [Arquitetura](#arquitetura)
- [Processamento de Dados](#processamento-de-dados)
- [Treinamento](#treinamento)
- [Teste e AvaliaÃ§Ã£o](#teste-e-avaliaÃ§Ã£o)
- [Weights & Biases](#weights--biases)
- [Ajuste de HiperparÃ¢metros](#ajuste-de-hiperparÃ¢metros)
- [FAQ](#faq)

---

## VisÃ£o Geral

### O que este mÃ³dulo faz?

O **Neural Ancestry Predictor** treina uma rede neural para prever a ancestralidade genÃ©tica de indivÃ­duos usando:

- **Entrada**: PrediÃ§Ãµes AlphaGenome (ex: ATAC-seq, RNA-seq) de janelas genÃ´micas
- **SaÃ­da**: SuperpopulaÃ§Ã£o (AFR, AMR, EAS, EUR, SAS), PopulaÃ§Ã£o (26 classes) ou FROG likelihood (150 valores)

### CaracterÃ­sticas

- âœ… **Totalmente configurÃ¡vel via YAML**
- âœ… **Suporta mÃºltiplos targets** (superpopulation, population, FROG likelihood)
- âœ… **Processamento flexÃ­vel** de janelas, haplÃ³tipos e outputs AlphaGenome
- âœ… **IntegraÃ§Ã£o com Weights & Biases** para tracking e visualizaÃ§Ã£o
- âœ… **Checkpointing automÃ¡tico** para salvar modelos treinados
- âœ… **MÃ©tricas detalhadas** (accuracy, precision, recall, F1, confusion matrix)
- âœ… **NormalizaÃ§Ã£o automÃ¡tica** com cache de parÃ¢metros

---

## InstalaÃ§Ã£o

### 1. DependÃªncias

```bash
# Navegar para o diretÃ³rio
cd genomics/neural_ancestry_predictor

# Instalar dependÃªncias Python
pip install torch numpy pandas pyyaml scikit-learn rich

# Opcional: Weights & Biases (para tracking)
pip install wandb
wandb login  # Autenticar com sua conta W&B
```

### 2. Dataset

Este mÃ³dulo requer um dataset PyTorch criado pelo `build_non_longevous_dataset`:

```bash
# Exemplo: dataset em /dados/GENOMICS_DATA/top3/non_longevous_results
# Deve conter:
#   - individuals/
#   - dataset_metadata.json
```

Consulte `build_non_longevous_dataset/docs/PYTORCH_DATASET.md` para mais informaÃ§Ãµes.

---

## Uso RÃ¡pido

### Treinar Modelo

```bash
cd neural_ancestry_predictor
python3 neural_ancestry_predictor.py --config configs/default.yaml
```

### Testar Modelo

```bash
python3 neural_ancestry_predictor.py --config configs/default.yaml --mode test
```

### Exemplo de SaÃ­da

```
â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ ðŸ§¬ Genomics                             â”‚
â”‚                                         â”‚
â”‚ Neural Ancestry Predictor               â”‚
â”‚ Modo: train                             â”‚
â”‚ Target: superpopulation                 â”‚
â”‚ Config: configs/default.yaml            â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

Device: cuda
[INFO] GenomicLongevityDataset inicializado:
  â€¢ Dataset: non_longevous_1000g
  â€¢ IndivÃ­duos: 78
  â€¢ Load predictions: True
  â€¢ Load sequences: False

Computando parÃ¢metros de normalizaÃ§Ã£o...
NormalizaÃ§Ã£o: mean=0.123456, std=0.654321

Dataset split:
  â€¢ Treino: 54 amostras
  â€¢ ValidaÃ§Ã£o: 12 amostras
  â€¢ Teste: 12 amostras

Modelo criado:
  â€¢ Input size: 11000
  â€¢ Hidden layers: [128, 64]
  â€¢ Output size: 5
  â€¢ Activation: relu
  â€¢ Dropout: 0.2
  â€¢ Total parameters: 1,415,237

â•­â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•®
â”‚ Iniciando Treinamento                   â”‚
â”‚                                         â”‚
â”‚ Ã‰pocas: 100                             â”‚
â”‚ Batch size: 16                          â”‚
â”‚ Learning rate: 0.001                    â”‚
â•°â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¯

ValidaÃ§Ã£o - Ã‰poca 5: Loss=0.8234, Accuracy=0.7500
âœ“ Checkpoint salvo: models/best_accuracy.pt

...

âœ“ Treinamento concluÃ­do!
```

---

## ConfiguraÃ§Ã£o

### Estrutura do Arquivo YAML

O arquivo `configs/default.yaml` contÃ©m **8 seÃ§Ãµes principais**:

#### A) Dataset Input Parameters

Controla **o que** carregar do dataset e **como** processar:

```yaml
dataset_input:
  dataset_dir: "/path/to/dataset"           # Caminho do dataset PyTorch
  alphagenome_outputs: ["ATAC"]             # Quais outputs usar (RNA_SEQ, ATAC, CAGE, etc.)
  haplotype_mode: "H1+H2"                   # "H1", "H2" ou "H1+H2"
  window_center_size: 100                   # Tamanho do trecho central (bases)
  downsample_factor: 1                      # Fator de downsampling (1 = sem)
```

**Impacto na Dimensionalidade:**

- Cada janela tem ~1M bases por output
- `window_center_size=100` â†’ extrai 100 bases do centro
- `downsample_factor=2` â†’ usa 1 a cada 2 bases (reduz para 50)
- `haplotype_mode="H1+H2"` â†’ dobra o tamanho (2 haplÃ³tipos)
- DimensÃ£o final = `n_windows Ã— n_outputs Ã— n_haplotypes Ã— (window_center_size / downsample_factor)`

**Exemplo:**
- 55 janelas (SNPs)
- 1 output (ATAC)
- 2 haplÃ³tipos (H1+H2)
- window_center_size=100, downsample_factor=1
- **DimensÃ£o = 55 Ã— 1 Ã— 2 Ã— 100 = 11,000 features**

#### B) Output Parameters

Define **o que** a rede deve prever:

```yaml
output:
  prediction_target: "superpopulation"  # "superpopulation", "population" ou "frog_likelihood"
```

| Target | Tipo | Classes | Dificuldade |
|--------|------|---------|-------------|
| `superpopulation` | ClassificaÃ§Ã£o | 5 (AFR, AMR, EAS, EUR, SAS) | FÃ¡cil â­ |
| `population` | ClassificaÃ§Ã£o | 26 | MÃ©dia â­â­ |
| `frog_likelihood` | RegressÃ£o | 150 valores | DifÃ­cil â­â­â­ |

#### C) Model Architecture

Define a **arquitetura** da rede:

```yaml
model:
  hidden_layers: [128, 64]    # Lista de neurÃ´nios por camada oculta
  activation: "relu"          # "relu", "tanh" ou "sigmoid"
  dropout_rate: 0.2           # Taxa de dropout (0.0 a 1.0)
```

**Arquitetura resultante:**

```
Input (11000) â†’ Dense(128) â†’ ReLU â†’ Dropout(0.2) â†’
Dense(64) â†’ ReLU â†’ Dropout(0.2) â†’
Dense(5) â†’ Softmax
```

#### D) Training Parameters

Controla o **processo de treinamento**:

```yaml
training:
  optimizer: "adam"              # "adam", "adamw" ou "sgd"
  learning_rate: 0.001           # Taxa de aprendizado
  loss_function: "cross_entropy" # "cross_entropy" ou "mse"
  batch_size: 16                 # NÃºmero de amostras por batch
  num_epochs: 100                # NÃºmero de Ã©pocas
  validation_frequency: 5        # Validar a cada N Ã©pocas
```

#### E) Data Split

Define **divisÃ£o do dataset**:

```yaml
data_split:
  train_split: 0.7      # 70% para treino
  val_split: 0.15       # 15% para validaÃ§Ã£o
  test_split: 0.15      # 15% para teste
  random_seed: 42       # Seed para reprodutibilidade
```

#### F) Weights & Biases

ConfiguraÃ§Ã£o de **tracking e visualizaÃ§Ã£o**:

```yaml
wandb:
  use_wandb: false                          # Habilitar W&B
  project_name: "neural-ancestry-predictor" # Nome do projeto
  run_name: null                            # Nome do run (auto se null)
  log_frequency: 10                         # Log a cada N batches
```

#### G) Checkpointing

Controla **salvamento de modelos**:

```yaml
checkpointing:
  checkpoint_dir: "models"       # DiretÃ³rio para checkpoints
  save_frequency: 10             # Salvar a cada N Ã©pocas
  load_checkpoint: null          # Caminho para checkpoint existente
```

#### H) Mode

Define **modo de operaÃ§Ã£o**:

```yaml
mode: "train"  # "train" ou "test"
```

---

## Arquitetura

### VisÃ£o Geral

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NEURAL ANCESTRY PREDICTOR                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  Input: AlphaGenome Predictions                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Window 1 (SNP/Gene)                      â”‚               â”‚
â”‚  â”‚  â”œâ”€ H1: [ATAC: 100 bases]                â”‚               â”‚
â”‚  â”‚  â””â”€ H2: [ATAC: 100 bases]                â”‚               â”‚
â”‚  â”‚ Window 2                                  â”‚               â”‚
â”‚  â”‚  â”œâ”€ H1: [ATAC: 100 bases]                â”‚               â”‚
â”‚  â”‚  â””â”€ H2: [ATAC: 100 bases]                â”‚               â”‚
â”‚  â”‚ ...                                       â”‚               â”‚
â”‚  â”‚ Window 55                                 â”‚               â”‚
â”‚  â”‚  â”œâ”€ H1: [ATAC: 100 bases]                â”‚               â”‚
â”‚  â”‚  â””â”€ H2: [ATAC: 100 bases]                â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚           â†“ ConcatenaÃ§Ã£o + NormalizaÃ§Ã£o                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Feature Vector [11000 elementos]         â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚           â†“                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Dense Layer (128 neurons)                â”‚               â”‚
â”‚  â”‚ ReLU Activation                          â”‚               â”‚
â”‚  â”‚ Dropout (0.2)                            â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚           â†“                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Dense Layer (64 neurons)                 â”‚               â”‚
â”‚  â”‚ ReLU Activation                          â”‚               â”‚
â”‚  â”‚ Dropout (0.2)                            â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚           â†“                                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚ Output Layer (5 neurons)                 â”‚               â”‚
â”‚  â”‚ Softmax                                  â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚           â†“                                                  â”‚
â”‚  Output: [AFR, AMR, EAS, EUR, SAS] probabilities           â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes

1. **ProcessedGenomicDataset**: Dataset wrapper que:
   - Carrega dados do `GenomicLongevityDataset`
   - Extrai trecho central das janelas
   - Aplica downsampling
   - Combina haplÃ³tipos
   - Normaliza (z-score)
   - Cache de parÃ¢metros de normalizaÃ§Ã£o

2. **AncestryPredictor**: Modelo PyTorch que:
   - ConstruÃ§Ã£o dinÃ¢mica baseada em config
   - Camadas totalmente conectadas (Dense)
   - Dropout para regularizaÃ§Ã£o
   - Softmax para classificaÃ§Ã£o ou linear para regressÃ£o

3. **Trainer**: Gerencia treinamento:
   - Loop de treino com progress bars
   - ValidaÃ§Ã£o periÃ³dica
   - Checkpointing automÃ¡tico
   - Logging no W&B

4. **Tester**: Gerencia teste:
   - InferÃªncia no conjunto de teste
   - MÃ©tricas detalhadas
   - Confusion matrix
   - Classification report

---

## Processamento de Dados

### Pipeline de Processamento

```
Dataset Original â†’ ExtraÃ§Ã£o â†’ Downsampling â†’ CombinaÃ§Ã£o â†’ NormalizaÃ§Ã£o â†’ Tensor
                   de Centro                   HaplÃ³tipos
```

#### 1. ExtraÃ§Ã£o de Trecho Central

Cada janela tem ~1M bases. ExtraÃ­mos o trecho central:

```python
# window_center_size = 100
# Array original: [1000000 elementos]
center_idx = 500000
start = center_idx - 50  # 499950
end = center_idx + 50    # 500050
extracted = array[start:end]  # [100 elementos]
```

**Por que centro?** Assume que regiÃ£o central Ã© mais relevante (prÃ³xima ao gene/SNP).

#### 2. Downsampling

Reduz ainda mais a dimensionalidade:

```python
# downsample_factor = 2
downsampled = extracted[::2]  # [50 elementos]
```

#### 3. CombinaÃ§Ã£o de HaplÃ³tipos

```python
# haplotype_mode = "H1+H2"
features = concatenate([H1_features, H2_features])

# haplotype_mode = "H1"
features = H1_features
```

#### 4. NormalizaÃ§Ã£o

NormalizaÃ§Ã£o z-score usando toda a base:

```python
# Computado uma vez no inÃ­cio
mean = mean(all_training_data)
std = std(all_training_data)

# Aplicado a cada amostra
normalized = (features - mean) / std
```

ParÃ¢metros salvos em `models/normalization_params.json` para reutilizaÃ§Ã£o.

---

## Treinamento

### Executar Treinamento

```bash
python3 neural_ancestry_predictor.py --config configs/default.yaml
```

### Durante o Treinamento

O programa irÃ¡:

1. **Carregar dataset** e imprimir sumÃ¡rio
2. **Computar normalizaÃ§Ã£o** (pode demorar alguns minutos)
3. **Dividir dados** em treino/validaÃ§Ã£o/teste
4. **Criar modelo** e imprimir arquitetura
5. **Treinar** com progress bars por Ã©poca
6. **Validar** a cada N Ã©pocas
7. **Salvar checkpoints**:
   - `best_loss.pt`: Melhor loss de validaÃ§Ã£o
   - `best_accuracy.pt`: Melhor accuracy de validaÃ§Ã£o
   - `epoch_N.pt`: Checkpoints periÃ³dicos

### Monitoramento

**Terminal:**
- Progress bars por Ã©poca
- Loss e accuracy de validaÃ§Ã£o
- Avisos e erros

**Arquivos:**
- `models/training_history.json`: HistÃ³rico completo
- `models/normalization_params.json`: ParÃ¢metros de normalizaÃ§Ã£o

**Weights & Biases** (se habilitado):
- GrÃ¡ficos de loss em tempo real
- MÃ©tricas de validaÃ§Ã£o
- Histogramas de gradientes
- ComparaÃ§Ã£o entre runs

### Continuar Treinamento

Para continuar de um checkpoint:

```yaml
# configs/default.yaml
checkpointing:
  load_checkpoint: "models/epoch_50.pt"
```

---

## Teste e AvaliaÃ§Ã£o

### Executar Teste

```bash
python3 neural_ancestry_predictor.py --config configs/default.yaml --mode test
```

Ou configure no YAML:

```yaml
mode: "test"
checkpointing:
  load_checkpoint: "models/best_accuracy.pt"
```

### Resultados

O teste gera:

**1. MÃ©tricas Gerais:**

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘        MÃ©tricas de Performance        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ Accuracy           â”‚ 0.9167           â•‘
â•‘ Precision (weighted)â”‚ 0.9250          â•‘
â•‘ Recall (weighted)  â”‚ 0.9167           â•‘
â•‘ F1-Score (weighted)â”‚ 0.9183           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

**2. Classification Report:**

```
              precision    recall  f1-score   support

         AFR       0.92      0.95      0.93        20
         AMR       0.88      0.85      0.86        13
         EAS       0.95      0.93      0.94        15
         EUR       0.90      0.93      0.91        15
         SAS       0.93      0.90      0.91        15

    accuracy                           0.92        78
   macro avg       0.92      0.91      0.91        78
weighted avg       0.92      0.92      0.92        78
```

**3. Confusion Matrix:**

```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              Confusion Matrix                  â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘ True \ Pred â”‚  AFR  â”‚  AMR  â”‚  EAS  â”‚  EUR  â”‚  SAS  â•‘
â•‘ AFR         â”‚   19  â”‚    1  â”‚    0  â”‚    0  â”‚    0  â•‘
â•‘ AMR         â”‚    1  â”‚   11  â”‚    0  â”‚    1  â”‚    0  â•‘
â•‘ EAS         â”‚    0  â”‚    0  â”‚   14  â”‚    1  â”‚    0  â•‘
â•‘ EUR         â”‚    0  â”‚    1  â”‚    0  â”‚   14  â”‚    0  â•‘
â•‘ SAS         â”‚    0  â”‚    0  â”‚    1  â”‚    0  â”‚   14  â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### InterpretaÃ§Ã£o

- **Accuracy**: % de prediÃ§Ãµes corretas
- **Precision**: % de prediÃ§Ãµes positivas corretas
- **Recall**: % de casos positivos identificados
- **F1-Score**: MÃ©dia harmÃ´nica de precision e recall
- **Confusion Matrix**: Onde o modelo erra

**Exemplo de anÃ¡lise:**
- AFR: Alta recall (0.95) â†’ identifica bem africanos
- AMR: Menor precision (0.88) â†’ Ã s vezes confunde com outras
- Diagonal forte â†’ modelo bem calibrado

---

## Weights & Biases

### Configurar W&B

```bash
# Instalar
pip install wandb

# Autenticar
wandb login

# Habilitar no config
```

```yaml
wandb:
  use_wandb: true
  project_name: "neural-ancestry-predictor"
  run_name: "experiment-atac-h1h2-100bases"  # Opcional
```

### VisualizaÃ§Ãµes DisponÃ­veis

1. **Loss Curves**: Train vs Validation loss
2. **Accuracy**: EvoluÃ§Ã£o da accuracy
3. **Confusion Matrix**: Matriz interativa
4. **Gradients**: Histogramas de gradientes
5. **Parameters**: DistribuiÃ§Ã£o de pesos
6. **System Metrics**: GPU, CPU, RAM

### Comparar Experimentos

No dashboard do W&B, vocÃª pode:
- Sobrepor grÃ¡ficos de mÃºltiplos runs
- Filtrar por hiperparÃ¢metros
- Gerar tabelas de comparaÃ§Ã£o
- Exportar grÃ¡ficos para papers (PNG, SVG, PDF)

---

## Ajuste de HiperparÃ¢metros

### Dimensionalidade muito alta?

**Problema**: Treino muito lento, memÃ³ria insuficiente

**SoluÃ§Ãµes**:
```yaml
dataset_input:
  window_center_size: 50        # Reduzir de 100 para 50
  downsample_factor: 2          # Usar 1 a cada 2 bases
  haplotype_mode: "H1"          # Usar apenas um haplÃ³tipo
  alphagenome_outputs: ["ATAC"] # Usar apenas 1 output
```

### Underfitting (loss alto)?

**Problema**: Modelo nÃ£o consegue aprender padrÃµes

**SoluÃ§Ãµes**:
```yaml
model:
  hidden_layers: [256, 128, 64]  # Mais camadas/neurÃ´nios
  dropout_rate: 0.0               # Remover regularizaÃ§Ã£o

training:
  num_epochs: 200                 # Treinar por mais tempo
```

### Overfitting (val_loss > train_loss)?

**Problema**: Modelo memoriza treino mas nÃ£o generaliza

**SoluÃ§Ãµes**:
```yaml
model:
  dropout_rate: 0.5               # Aumentar dropout
  hidden_layers: [64, 32]         # Reduzir capacidade

training:
  learning_rate: 0.0001           # Learning rate menor
```

### Treino instÃ¡vel (loss oscila)?

**Problema**: Gradientes instÃ¡veis

**SoluÃ§Ãµes**:
```yaml
training:
  learning_rate: 0.0001           # LR menor
  optimizer: "adamw"              # Tentar outro otimizador
  batch_size: 32                  # Batch maior
```

### ConvergÃªncia lenta?

**Problema**: Treino demora muito

**SoluÃ§Ãµes**:
```yaml
training:
  learning_rate: 0.01             # LR maior (cuidado!)
  batch_size: 64                  # Batch maior
  optimizer: "adam"               # Adam geralmente mais rÃ¡pido que SGD
```

---

## FAQ

### Q: Quanto tempo leva o treinamento?

**A**: Depende de:
- **Dataset size**: 78 amostras â†’ minutos; 1000 amostras â†’ horas
- **Dimensionalidade**: 100 bases â†’ rÃ¡pido; 10000 bases â†’ lento
- **Hardware**: GPU â†’ 10-100x mais rÃ¡pido que CPU
- **Ã‰pocas**: 100 Ã©pocas â†’ proporcional

**Estimativas** (78 amostras, 100 bases, GPU):
- NormalizaÃ§Ã£o: ~30 segundos
- Ã‰poca: ~5 segundos
- 100 Ã©pocas: ~8 minutos

### Q: Qual target devo usar?

**A**: RecomendaÃ§Ãµes:
- **Iniciante**: `superpopulation` (5 classes, mais fÃ¡cil)
- **IntermediÃ¡rio**: `population` (26 classes)
- **AvanÃ§ado**: `frog_likelihood` (regressÃ£o, 150 valores)

### Q: Preciso de GPU?

**A**: NÃ£o Ã© obrigatÃ³rio, mas **altamente recomendado**:
- CPU: Funciona, mas ~50-100x mais lento
- GPU: Nvidia com CUDA (RTX 3060 ou superior ideal)

Para instalar PyTorch com GPU:
```bash
# CUDA 11.8
pip install torch --index-url https://download.pytorch.org/whl/cu118
```

### Q: Como interpretar a accuracy?

**A**: Depende do baseline:
- **Random guessing** (5 classes): 20%
- **Bom modelo**: 70-80%
- **Excelente modelo**: 85-95%
- **Perfeito**: 100% (cuidado com overfitting!)

Compare sempre com validaÃ§Ã£o E teste.

### Q: Posso usar mÃºltiplos outputs AlphaGenome?

**A**: Sim! Aumenta dimensionalidade mas pode melhorar desempenho:

```yaml
dataset_input:
  alphagenome_outputs:
    - "ATAC"
    - "RNA_SEQ"
    - "CAGE"
```

DimensÃ£o cresce linearmente: 1 output â†’ 11k features; 3 outputs â†’ 33k features.

### Q: Como adicionar mais genes/SNPs ao dataset?

**A**: Recrie o dataset com `build_non_longevous_dataset`:

```yaml
# build_non_longevous_dataset/configs/custom.yaml
build_window_params:
  mode: "snp"
  snp:
    snp_list_file: "path/to/your_snps.txt"  # Adicionar mais SNPs
```

Mais janelas â†’ maior dimensionalidade â†’ pode melhorar ou piorar (curse of dimensionality).

### Q: Erro "CUDA out of memory"?

**A**: Reduza uso de memÃ³ria:

```yaml
training:
  batch_size: 4          # Reduzir batch
  
dataset_input:
  window_center_size: 50  # Reduzir dimensÃ£o
  downsample_factor: 2    # Aumentar downsampling
```

Ou use CPU:
```bash
# ForÃ§ar CPU
export CUDA_VISIBLE_DEVICES=""
python3 neural_ancestry_predictor.py --config configs/default.yaml
```

### Q: Como exportar grÃ¡ficos para paper?

**A**: 

**OpÃ§Ã£o 1: Weights & Biases**
- No dashboard, clicar em grÃ¡fico â†’ "Export" â†’ PNG/SVG
- Alta qualidade, ideal para publicaÃ§Ã£o

**OpÃ§Ã£o 2: Programaticamente**
```python
import matplotlib.pyplot as plt
import json

# Carregar histÃ³rico
with open('models/training_history.json') as f:
    history = json.load(f)

# Plotar
plt.figure(figsize=(10, 6), dpi=300)
plt.plot(history['epoch'], history['train_loss'], label='Train')
plt.plot(history['epoch'], history['val_loss'], label='Validation')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.savefig('loss_curve.png', dpi=300, bbox_inches='tight')
```

---

## Estrutura de Arquivos

```
neural_ancestry_predictor/
â”œâ”€â”€ neural_ancestry_predictor.py    # Programa principal
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ default.yaml                 # ConfiguraÃ§Ã£o padrÃ£o
â”œâ”€â”€ models/                          # Checkpoints (criado automaticamente)
â”‚   â”œâ”€â”€ best_loss.pt
â”‚   â”œâ”€â”€ best_accuracy.pt
â”‚   â”œâ”€â”€ epoch_10.pt
â”‚   â”œâ”€â”€ epoch_20.pt
â”‚   â”œâ”€â”€ normalization_params.json
â”‚   â””â”€â”€ training_history.json
â””â”€â”€ README.md                        # Esta documentaÃ§Ã£o
```

---

## ReferÃªncias

- **PyTorch**: https://pytorch.org/
- **Weights & Biases**: https://wandb.ai/
- **AlphaGenome**: https://alphagenome.ai/
- **1000 Genomes**: http://www.internationalgenome.org/
- **FROGAncestryCalc**: Ancestry inference via AISNPs

---

## Suporte

Para problemas ou questÃµes:

1. Verifique este README
2. Consulte `build_non_longevous_dataset/docs/PYTORCH_DATASET.md`
3. Execute com modo debug: adicione prints no cÃ³digo
4. Verifique logs do W&B (se habilitado)

**Autor**: Alberto F. De Souza (via ChatGPT)  
**Data**: 2025-11-14  
**VersÃ£o**: 1.0

---

## Changelog

### v1.0 (2025-11-14)
- âœ¨ ImplementaÃ§Ã£o inicial
- âœ¨ Suporte para superpopulation, population e FROG likelihood
- âœ¨ IntegraÃ§Ã£o com Weights & Biases
- âœ¨ Processamento configurÃ¡vel de janelas e haplÃ³tipos
- âœ¨ Checkpointing e normalizaÃ§Ã£o automÃ¡tica
- âœ¨ MÃ©tricas detalhadas e visualizaÃ§Ãµes

