# ═══════════════════════════════════════════════════════════════════
# Neural Ancestry Predictor - Default Configuration
# ═══════════════════════════════════════════════════════════════════
#
# Este arquivo configura a rede neural para predição de ancestralidade
# a partir de predições AlphaGenome armazenadas no dataset PyTorch.
#
# Uso:
#   python3 neural_ancestry_predictor.py --config configs/default.yaml
#
# ═══════════════════════════════════════════════════════════════════

# ───────────────────────────────────────────────────────────────────
# A) DATASET INPUT PARAMETERS
# ───────────────────────────────────────────────────────────────────
# Configurações de entrada do dataset

dataset_input:
  # Caminho para o diretório do dataset PyTorch
  # Deve conter individuals/ e dataset_metadata.json
  dataset_dir: "/dados/GENOMICS_DATA/top3/non_longevous_results"
  
  # Lista de outputs AlphaGenome a usar como entrada
  # Opções disponíveis: rna_seq, atac, cage, chip_histone, chip_tf, dnase, etc.
  # IMPORTANTE: Nomes devem estar em minúsculas (mesmo formato dos arquivos .npz)
  # Quanto mais outputs, maior a dimensão de entrada
  alphagenome_outputs:
    - "rna_seq"
  
  # Modo de combinação de haplótipos
  # Opções: "H1", "H2", "H1+H2"
  # - "H1": Usa apenas haplótipo 1
  # - "H2": Usa apenas haplótipo 2
  # - "H1+H2": Concatena ambos os haplótipos (dobra tamanho de entrada)
  haplotype_mode: "H1"
  
  # Tamanho do trecho central (em bases) a extrair de cada janela
  # Cada janela tem ~1M bases. Este parâmetro seleciona o trecho central.
  # Valores menores = menos features, treinamento mais rápido
  # Valores maiores = mais informação, maior dimensionalidade
  # Default: 100 bases (bem pequeno para começar)
  window_center_size: 50000
  
  # Fator de downsampling a aplicar após extração do trecho central
  # 1 = usar todas as bases selecionadas (sem downsampling)
  # 2 = usar 1 a cada 2 bases
  # 10 = usar 1 a cada 10 bases
  # Maior downsampling = menor dimensionalidade, mais rápido, menos informação
  downsample_factor: 1
  
  # Método de normalização dos dados
  # Opções:
  #   "zscore": Normalização Z-score padrão (mean=0, std=1)
  #             Usa mean e std globais de todos os dados
  #             Pode ter problemas com outliers extremos
  #
  #   "minmax_keep_zero": Min-Max mantendo zeros
  #             Zeros permanecem zeros (importante para dados esparsos)
  #             Valores não-zero são divididos pelo máximo não-zero
  #             Resultado: valores em [0, 1]
  #             Ideal para dados com muitos zeros (ATAC-seq, RNA-seq)
  #
  #   "log": Normalização logarítmica com log1p
  #             Aplica log1p (log(1+x)) e divide pelo log1p(max)
  #             Zeros permanecem zeros automaticamente
  #             Comprime outliers extremos
  #             Ideal para dados com distribuição exponencial/lognormal
  #
  # Recomendado: "minmax_keep_zero" ou "log" para dados AlphaGenome
  normalization_method: "log"
  
  # Valor pré-definido para normalização (otimização de performance)
  # Se 0 ou null: Computa automaticamente (demorado, ~30s-60s)
  # Se diferente de 0: Usa este valor diretamente (instantâneo)
  # 
  # IMPORTANTE: Só funciona com métodos "log" e "minmax_keep_zero"
  # Para "zscore" sempre computa (precisa mean e std)
  # 
  # Como usar:
  # 1. Execute uma vez com 0 para computar
  # 2. Veja o valor no log (ex: "log1p(max): 4.337291")
  # 3. Configure aqui para próximas execuções
  # 
  # Valores típicos (ATAC-seq, 11 windows):
  #   - log: ~4.33 a 4.34
  #   - minmax_keep_zero: varia mais (ver log)
  normalization_value: 0.0
  
  # Diretório para cache do dataset processado (normalizado e dividido)
  # Se null, não usa cache (sempre reprocessa os dados)
  # Se especificado:
  #   - Se o cache existir e for válido, carrega de lá (economia de tempo)
  #   - Se não existir, processa os dados e salva no cache
  # O cache inclui: dados normalizados, splits de treino/val/teste, e metadados
  # Ideal para múltiplas execuções com mesmo dataset e mesmos parâmetros
  # null = desabilita cache; "processed_datasets/default_cache" = habilita
  processed_cache_dir: /dados/GENOMICS_DATA/top3/non_longevous_results_runs

# ───────────────────────────────────────────────────────────────────
# B) OUTPUT PARAMETERS
# ───────────────────────────────────────────────────────────────────
# Configurações do target de predição

output:
  # Target de predição
  # Opções:
  #   "superpopulation": Prediz superpopulação (5 classes: AFR, AMR, EAS, EUR, SAS)
  #   "population": Prediz população (26 classes)
  #   "frog_likelihood": Prediz likelihood FROG (150 valores - regressão)
  prediction_target: "superpopulation"
  
  # Classes conhecidas (otimização de performance)
  # Se especificado, usa estas classes diretamente sem escanear o dataset (~30s economia)
  # Se null ou vazio, escaneia o dataset para descobrir classes automaticamente
  # 
  # Para superpopulation (5 classes):
  known_classes: ["AFR", "AMR", "EAS", "EUR", "SAS"]
  # 
  # Para population (26 classes), descomente e complete:
  # known_classes: ["ACB", "ASW", "BEB", "CDX", "CEU", "CHB", "CHS", "CLM", 
  #                 "ESN", "FIN", "GBR", "GIH", "GWD", "IBS", "ITU", "JPT", 
  #                 "KHV", "LWK", "MSL", "MXL", "PEL", "PJL", "PUR", "STU", 
  #                 "TSI", "YRI"]
  # 
  # Para frog_likelihood (regressão), deixe como null:
  # known_classes: null

# ───────────────────────────────────────────────────────────────────
# C) MODEL ARCHITECTURE
# ───────────────────────────────────────────────────────────────────
# Configurações da arquitetura da rede neural

model:
  # Tipo de rede neural
  # Opções:
  #   "NN": Rede neural totalmente conectada (fully connected)
  #   "CNN": Rede neural convolucional (convolutional)
  type: "CNN"
  
  # Camadas ocultas: lista com número de neurônios em cada camada
  # Exemplo: [256, 128, 64] = 3 camadas ocultas
  # - As camadas exceto a ÚLTIMA usam activation configurável (relu/tanh/sigmoid)
  # - A ÚLTIMA camada é sempre LINEAR (pré-softmax) automaticamente
  # - Para 5 classes de saída, recomendado: última camada >= 32 neurônios
  # 
  # Camada de entrada é determinada automaticamente pelos dados
  # Camada de saída é determinada pelo prediction_target
  hidden_layers:
    - 100
    - 40
    # - 64
  
  # Função de ativação para camadas ocultas INTERMEDIÁRIAS apenas
  # (a última hidden layer é sempre linear antes do softmax)
  # Opções: "relu", "tanh", "sigmoid"
  activation: "relu"
  
  # Taxa de dropout (0.0 a 1.0)
  # 0.0 = sem dropout
  # Valores típicos: 0.2 a 0.5
  # Ajuda a prevenir overfitting
  dropout_rate: 0.2
  
  # Parâmetros específicos para CNN (ignorados se type='NN')
  cnn:
    # Tamanho do kernel de convolução [height, width]
    # Exemplos:
    #   [5, 5]  - Kernel 5x5 (convolução 2D, mistura linhas e colunas)
    #   [1, 20] - Kernel 1x20 (convolução 1D sobre cada linha/SNP, não mistura linhas)
    #   [3, 1]  - Kernel 3x1 (convolução 1D vertical, mistura 3 SNPs adjacentes)
    kernel_size: [1, 20]
    
    # Número de filtros/kernels convolucionais
    # Cada filtro aprende um padrão diferente
    # Mais filtros = mais features detectadas, mais parâmetros
    num_filters: 5
    
    # Stride (passo) da convolução - pode ser escalar ou lista [vertical, horizontal]
    # Escalar: 20 = stride 20 em ambas dimensões
    # Lista: [1, 20] = stride 1 vertical (preserva linhas), stride 20 horizontal
    # 
    # Exemplos:
    #   1 ou [1, 1]   - Mover 1 posição por vez (overlap máximo)
    #   [1, 20]       - Para kernel [1, 20]: sem overlap (cada posição processada 1x)
    #   [2, 2]        - Redução agressiva em ambas dimensões
    stride: [1, 20]
    
    # Padding - pode ser escalar ou lista [vertical, horizontal]
    # Adiciona zeros nas bordas da entrada antes da convolução
    # 
    # Escalar: 0 = sem padding em nenhuma dimensão
    # Escalar: 1 = padding de 1 pixel em todas as bordas
    # Lista: [0, 10] = sem padding vertical, padding 10 horizontal
    # 
    # Fórmula de dimensão de saída:
    #   out_size = (in_size + 2*padding - kernel_size) / stride + 1
    # 
    # Para manter dimensão original (quando stride=1):
    #   padding = (kernel_size - 1) / 2
    #   Exemplo: kernel 5x5, stride 1 → padding 2 mantém dimensão
    #   Exemplo: kernel [1, 20], stride [1, 1] → padding [0, 9] mantém dimensão horizontal
    # 
    # Valores comuns:
    #   0 ou [0, 0]  - Sem padding (dimensão reduz)
    #   1 ou [1, 1]  - Padding mínimo
    #   [0, 9]       - Para kernel [1, 20]: sem padding vertical, preserva ~dimensão horizontal
    padding: 0
    
    # Tamanho do pooling [height, width] ou null para desabilitar
    # Reduz dimensionalidade após convolução através de MaxPooling
    # 
    # Exemplos:
    #   null        - Sem pooling (mantém dimensão pós-convolução)
    #   [2, 2]      - MaxPool 2x2 (reduz ambas dimensões pela metade)
    #   [1, 2]      - MaxPool vertical 1, horizontal 2 (reduz só largura)
    # 
    # ATENÇÃO: Pooling pode causar perda de informação, use com cautela
    pool_size: null

# ───────────────────────────────────────────────────────────────────
# D) TRAINING PARAMETERS
# ───────────────────────────────────────────────────────────────────
# Configurações de treinamento

training:
  # Otimizador
  # Opções: "adam", "adamw", "sgd"
  optimizer: "adam"
  
  # Taxa de aprendizado (learning rate)
  # Valores típicos: 0.001 (1e-3) a 0.0001 (1e-4)
  learning_rate: 0.001
  
  # Função de loss
  # "cross_entropy": Para classificação (superpopulation, population)
  # "mse": Para regressão (frog_likelihood)
  loss_function: "cross_entropy"
  
  # Tamanho do batch (número de indivíduos por batch)
  # Valores maiores = treinamento mais estável, mais memória
  # Valores menores = mais atualizações, menos memória
  # Deve ser ajustado de acordo com tamanho do dataset
  batch_size: 1
  
  # Número de épocas de treinamento
  # Uma época = passar por todo o dataset de treino uma vez
  num_epochs: 500
  
  # Frequência de validação (em épocas)
  # A cada N épocas, executa validação
  validation_frequency: 1
  
  # ─────────────────────────────────────────────────────────────────
  # Learning Rate Scheduler
  # ─────────────────────────────────────────────────────────────────
  # Ajusta a taxa de aprendizado durante o treinamento
  
  lr_scheduler:
    # Habilitar scheduler
    enabled: true
    
    # Tipo de scheduler
    # "plateau": Reduz quando métrica para de melhorar (recomendado)
    # "step": Reduz em intervalos fixos de épocas
    # "cosine": Redução suave seguindo curva cosseno
    # "exponential": Redução exponencial constante
    # "multistep": Reduz em épocas específicas
    type: "plateau"
    
    # ─── Configurações para ReduceLROnPlateau ───
    # Monitora melhoria de loss ("min") ou accuracy ("max")
    mode: "min"
    
    # Fator de redução (novo_lr = lr * factor)
    factor: 0.5
    
    # Número de épocas sem melhora antes de reduzir
    patience: 10
    
    # Learning rate mínimo (não reduz abaixo disso)
    min_lr: 0.000001
    
    # ─── Configurações para StepLR ───
    # Reduz a cada N épocas
    step_size: 30
    
    # Fator de redução para StepLR
    # gamma: 0.1
    
    # ─── Configurações para CosineAnnealingLR ───
    # Número de épocas para completar o ciclo
    # T_max: 100
    
    # LR mínimo no final do ciclo
    # eta_min: 0.000001
    
    # ─── Configurações para ExponentialLR ───
    # Fator de decaimento por época (lr = lr * gamma)
    # gamma: 0.95
    
    # ─── Configurações para MultiStepLR ───
    # Épocas onde reduzir o LR
    # milestones: [30, 60, 90]
    # gamma: 0.1

# ───────────────────────────────────────────────────────────────────
# E) DATA SPLIT
# ───────────────────────────────────────────────────────────────────
# Configurações de divisão do dataset

data_split:
  # Fração dos dados para treinamento (0.0 a 1.0)
  train_split: 0.7
  
  # Fração dos dados para validação (0.0 a 1.0)
  val_split: 0.15
  
  # Fração dos dados para teste (0.0 a 1.0)
  # Nota: train_split + val_split + test_split deve ser <= 1.0
  test_split: 0.15
  
  # Seed para reprodutibilidade
  # Use o mesmo seed para ter sempre a mesma divisão
  # null = aleatório
  random_seed: 42

# ───────────────────────────────────────────────────────────────────
# F) WEIGHTS & BIASES
# ───────────────────────────────────────────────────────────────────
# Configurações de tracking e visualização com W&B

wandb:
  # Habilitar Weights & Biases
  # true = loga métricas, gráficos, confusion matrix no W&B
  # false = não usa W&B
  # Requer: pip install wandb && wandb login
  use_wandb: true
  
  # Nome do projeto no W&B
  project_name: "neural-ancestry-predictor"
  
  # Nome do run (opcional)
  # null = W&B gera nome automaticamente
  run_name: null
  
  # Frequência de logging (em batches)
  # Log a cada N batches de treino
  log_frequency: 50

# ───────────────────────────────────────────────────────────────────
# G) CHECKPOINTING
# ───────────────────────────────────────────────────────────────────
# Configurações de salvamento de checkpoints

checkpointing:
  # Diretório para salvar checkpoints
  checkpoint_dir: "models"
  
  # Salvar checkpoints durante o treinamento
  # true: Salva best_accuracy.pt, best_loss.pt e epoch_N.pt durante o treino + final.pt no fim
  # false: Salva APENAS final.pt no fim do treinamento (máxima economia de espaço em disco)
  save_during_training: true
  
  # Frequência de salvamento (em épocas)
  # Salva checkpoint a cada N épocas (apenas se save_during_training = true)
  save_frequency: 100
  
  # Caminho para checkpoint existente (para continuar treino ou testar)
  # null = começar do zero
  # Exemplo: "models/best_accuracy.pt"
  load_checkpoint: null

# ───────────────────────────────────────────────────────────────────
# H) DEBUG/VISUALIZATION
# ───────────────────────────────────────────────────────────────────
# Opções de debug e visualização

debug:
  # Habilitar visualização interativa dos dados durante treinamento
  # true = mostra gráfico com entrada/saída após cada batch (pressione Enter para continuar)
  # false = treino normal sem visualização
  # IMPORTANTE: Quando true, força batch_size=1 automaticamente
  enable_visualization: false
  
  # Número máximo de amostras a visualizar por época
  # null = visualiza todas as amostras da época
  # N = visualiza apenas as primeiras N amostras
  max_samples_per_epoch: null
  
  # Parâmetros de visualização da entrada 2D
  visualization:
    # Dimensões para rescale da imagem de entrada (apenas para visualização)
    # A rede recebe os dados na forma 2D original, não rescalada
    width: 600   # Largura da imagem visualizada
    height: 300  # Altura da imagem visualizada
  
  # Tainting de amostras para debug
  # Marca amostras com valor sentinela (-2) em posições específicas
  # baseadas na classe de saída, para verificar se rede aprende corretamente
  # Fórmula: posição = classe * (input_vector_size / num_classes)
  # Exemplo: 5 classes, 5500 features -> classe 0→pos 0, classe 1→pos 1100, etc.
  taint_at_cache_save: false  # Taintar ao salvar no cache (dados salvos já vêm marcados)
  taint_at_runtime: false      # Taintar durante __getitem__ (marca em tempo de execução)

# ───────────────────────────────────────────────────────────────────
# I) MODE
# ───────────────────────────────────────────────────────────────────
# Modo de operação

# Modo de execução
# "train": Treina a rede neural
# "test": Carrega checkpoint e testa no conjunto especificado em test_dataset
# Pode ser sobrescrito via --mode na linha de comando
mode: "train"

# Conjunto de dados a ser usado no modo test
# "test": Conjunto de teste (padrão)
# "train": Conjunto de treinamento
# "val": Conjunto de validação
test_dataset: "test"

# ═══════════════════════════════════════════════════════════════════
# NOTAS DE USO
# ═══════════════════════════════════════════════════════════════════
#
# 1. Ajuste de Dimensionalidade:
#    - Para reduzir tempo de treino: aumente downsample_factor ou reduza window_center_size
#    - Para mais informação: aumente window_center_size ou adicione mais alphagenome_outputs
#
# 2. Ajuste de Capacidade do Modelo:
#    - Underfitting (loss alto): aumente hidden_layers ou neurônios
#    - Overfitting (val_loss > train_loss): adicione dropout, reduza neurônios
#
# 3. Ajuste de Convergência:
#    - Treino instável: reduza learning_rate
#    - Treino muito lento: aumente learning_rate ou batch_size
#
# 4. Targets de Predição:
#    - superpopulation: Mais fácil (5 classes), bom para começar
#    - population: Mais difícil (26 classes), requer mais dados/capacidade
#    - frog_likelihood: Regressão (150 valores), mais difícil ainda
#
# 5. Haplotype Mode:
#    - H1 ou H2: Menor dimensionalidade, mais rápido
#    - H1+H2: Mais informação (diferenças entre haplótipos), dobra dimensão
#
# ═══════════════════════════════════════════════════════════════════

